<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NanoVLM-Lab: Democratizing Vision-Language Model Training for Everyone - Blog</title>
    <link rel="stylesheet" href="https://akash-kamalesh.github.io/style.css">
</head>
<body>
    <!-- Navigation Bar -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="https://akash-kamalesh.github.io" class="nav-logo">Akash Kamalesh</a>
            <ul class="nav-menu">
                <li class="nav-item">
                    <a href="https://akash-kamalesh.github.io#about" class="nav-link">About</a>
                </li>
                <li class="nav-item">
                    <a href="https://akash-kamalesh.github.io#publications" class="nav-link">Publications</a>
                </li>
                <li class="nav-item">
                    <a href="https:&#x2F;&#x2F;drive.google.com&#x2F;file&#x2F;d&#x2F;1d4vC6TQfj081uj_moJvR9mB1Fl0kJqEB&#x2F;view?usp=sharing" class="nav-link" target="_blank">Resume</a>
                </li>
                <li class="nav-item">
                    <a href="https://akash-kamalesh.github.io/blog" class="nav-link">Blog</a>
                </li>
                <li class="nav-item">
                    <a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?user=QUNjegoAAAAJ&amp;hl=en" class="nav-link" target="_blank" title="Google Scholar">
                        <img src="https://akash-kamalesh.github.io/google-scholar-brands-solid-full.svg" alt="Google Scholar" class="icon-img">
                    </a>
                </li>
                <li class="nav-item">
                    <a href="https:&#x2F;&#x2F;github.com&#x2F;akash-kamalesh" class="nav-link" target="_blank" title="GitHub">
                        <svg class="icon" viewBox="0 0 24 24" fill="currentColor">
                            <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v 3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a href="https:&#x2F;&#x2F;linkedin.com&#x2F;in&#x2F;akash-kamalesh" class="nav-link" target="_blank" title="LinkedIn">
                        <svg class="icon" viewBox="0 0 24 24" fill="currentColor">
                            <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.475-2.236-1.986-2.236-1.081 0-1.722.731-2.004 1.438-.103.249-.129.597-.129.946v5.421h-3.554s.05-8.736 0-9.646h3.554v1.364c.429-.659 1.196-1.597 2.905-1.597 2.12 0 3.71 1.386 3.71 4.365v5.514zM5.337 9.433c-1.144 0-1.915-.758-1.915-1.707 0-.955.77-1.708 1.963-1.708 1.192 0 1.915.753 1.94 1.708 0 .949-.748 1.707-1.988 1.707zm1.582 11.019H3.656V9.806h3.263v10.646zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.225 0z"/>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a href="mailto:akash.kamalesh03@gmail.com" class="nav-link" title="Email">
                        <svg class="icon" viewBox="0 0 24 24" fill="currentColor">
                            <path d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/>
                        </svg>
                    </a>
                </li>
            </ul>
        </div>
    </nav>

    <!-- Main Content -->
    <main id="top">
        
    <article class="blog-post">
        <div class="container">
            <header class="blog-post-header">
                <h1 class="blog-post-title">NanoVLM-Lab: Democratizing Vision-Language Model Training for Everyone</h1>
                <p class="blog-post-date"><strong>2025-12-17</strong></p>
                
                    <p class="blog-post-description">Exploring methods to build scalable and efficient vision-language models with a focus on representation learning and multimodal alignment.</p>
                
            </header>
            <div class="blog-post-content">
                <img src="/nanovlm_lab_cropped.png" alt="Logo" width="500" height="500" style="display: block; margin: 0 auto;">
<h2 id="introduction">Introduction</h2>
<p>A few months ago, I was an undergraduate student working on a research project to improve dual encoder models like CLIP, with a specific focus on visual reasoning capabilities. It was exciting work-the kind that makes you believe you're onto something extremely impressive. But there was a problem I ran into very quickly- the infamous <code>OutOfMemoryError</code>. I could reduce it by dropping the batch size to 4, but then I'd have to explain to reviewers why my results came from training with such an unusually small batch size.</p>
<p>Being a self-funded undergrad student, I quickly realized that experimenting with competitive open-source vision-language models such as Qwen-2.5, Gemma 3 etc. on free-tier GPUs was a non-starter. The computational demands were simply too high and/or the experience of running inference on free-tier GPUs wasn't great. So, I turned to cloud GPUs, hoping to unlock the ability to train and iterate on my ideas. But here's what I discovered: <strong>the real bottleneck wasn't just the GPU itself- it was everything that happened before the GPU was actually used.</strong></p>
<p>Data preprocessing, validation, experiment tracking, checkpoint management-these tasks consumed enormous amounts of time. And during all that time, the GPU sat idle while I was debugging data pipelines or validating data. But the meter kept running. I realised I was spending money on compute that wasn't actually being used for training.</p>
<p>In addition to compute, having spoken to researchers at NeurIPS last year, I noticed an increase in studies focused on efficient and more performative foundation models. The narrative has shifted a lot towards producing smaller, more compact and performative models. I wanted to create a project that would take care of the heavy lifting-all those tedious, time-consuming processes that could eat up hours. My goal was simple: abstract away the complexity with simple, helpful wrappers so that researchers and practitioners could focus on what actually matters: their ideas and their data.</p>
<p>One of the projects released as a result of this is a comprehensive training framework, <strong>NanoVLM-Lab</strong> designed to make vision-language model training accessible to anyone with limited resources. All you need is a dataset and a use-case. Whether you're working with a single free-tier GPU, a modest cloud instance, or even just a laptop for prototyping, NanoVLM-Lab is designed with a focus on supporting small VLMs so you can focus on the research.</p>
<p>I built this project keeping in mind independent researchers, students, and practitioners who are experimenting with complex architecture modifications, validating hypotheses, exploring new approaches, or building proof-of-concepts. You can safely construct your solutions on a small model, test the relative performance improvements and then reliably adapt it to the larger models such as Qwen 2.5-VL. This way, you can demonstrate your methods work reliably at scale.</p>
<hr />
<h2 id="the-problem-ideas-but-a-lack-of-resources">The Problem: Ideas but A Lack of Resources</h2>
<p>The current landscape of ML research has a significant barrier to entry: <strong>computational resources</strong>. Training large vision-language models requires multiple high-end GPUs, making it inaccessible to most small-scale researchers and practitioners from academia unless they are supplemented with a dedicated cluster or financial grant.</p>
<p>But here's the thing: <strong>you don't need a massive model to validate your ideas.</strong></p>
<p>Small, efficient models like nanoVLM can:</p>
<ul>
<li>Run on a single T4 GPU (or even CPU for inference)</li>
<li>Train on consumer hardware in hours instead of days</li>
<li>Achieve competitive performance on many tasks</li>
<li>Be deployed on edge devices and mobile platforms</li>
</ul>
<p><strong>A note on scale</strong>: While NanoVLM-Lab is designed to work on a single T4 GPU, it's important to understand the trade-offs. You can train with a single batch size and gradient accumulation steps on a single T4, which is great for experimentation and validation. However, if you're planning to use these models in production at scale, you'll need to scale up your compute accordingly. The framework supports this scaling, but single-GPU training is optimized for research and prototyping, not production-level deployments. Think of it as a way to validate your ideas and prove the concept before investing in larger infrastructure.</p>
<hr />
<h2 id="nanovlm-lab">NanoVLM-Lab</h2>
<p><img src="/flow_diagram.png" alt="Training Flow" /></p>
<p>NanoVLM-Lab follows a progressive training pipeline designed to unlock the full potential of vision-language models. You start with either a pre-trained nanoVLM model or initialize a new one from scratch. From there, the framework guides you through <strong>Supervised Fine-Tuning (SFT)</strong>, where you train your model on labeled image-text pairs to adapt it to your specific domain. This is the foundation—fast, straightforward, and effective for most use cases. During SFT, the model learns to structure its reasoning process and answers within special tokens like <code>&lt;think&gt;</code> and <code>&lt;/think&gt;</code>, enabling more coherent and reasoned outputs.</p>
<p>But SFT is just the beginning. Once you have a solid foundation, you can go beyond supervised learning and enter the world of <strong>preference tuning</strong> with either <strong>Direct Preference Optimization (DPO)</strong> or <strong>Group Relative Policy Optimization (GRPO)</strong>. DPO aligns your model with human preferences without needing a separate reward model—this is where things get truly interesting. GRPO takes it further, offering advanced policy optimization with custom reward functions for researchers who want fine-grained control over training dynamics. All three approaches are built on HuggingFace's Transformers and TRL, ensuring compatibility and reliability across the entire pipeline.</p>
<hr />
<h2 id="example-preference-tuning-with-dpo">Example: Preference-tuning with DPO</h2>
<p>Here's an example of how DPO preference tuning transforms nanoVLM's outputs:</p>
<table><thead><tr><th>Image</th><th>Prompt</th><th>Pre-trained nanoVLM Output*</th><th>DPO Preference-Tuned Output**</th></tr></thead><tbody>
<tr><td><img src="/cake.jpg" alt="Cake" /></td><td>"Why are cakes usually eaten at party's?"</td><td>"birthday"</td><td>"Cakes are usually eaten at parties to celebrate and to commemorate special occasions, such as birthdays, anniversaries, or holidays. The festive atmosphere and shared enjoyment create the ideal setting for cakes to be a highlight of the event."</td></tr>
</tbody></table>
<p>*Using the checkpoint: lusxvr/nanoVLM-230M-8k.<br>
**After short training on a subset of the dataset using DPO preference optimization.</p>
<p><strong>What's happening here?</strong></p>
<p>The pre-trained model gives a one-word answer: "birthday." It's not wrong, but it's not very useful. After just a few hours of DPO training on a subset of the dataset, the same model produces a thoughtful, aligned and comprehensive answer that actually explains the reasoning.</p>
<p>This is the magic of preference optimization: <strong>you're not just teaching the model facts, you're teaching it how to think and communicate.</strong></p>
<hr />
<h2 id="why-this-matters-for-the-community">Why This Matters for the Community</h2>
<h3 id="1-democratizing-research">1. <strong>Democratizing Research</strong></h3>
<p>Not everyone has access to a cluster of A100 GPUs. NanoVLM-Lab means that brilliant ideas from researchers with limited resources can now be validated and explored.</p>
<h3 id="2-rapid-experimentation">2. <strong>Rapid Experimentation</strong></h3>
<p>With consumer GPUs, you can now:</p>
<ul>
<li>Train a prototypical SFT model in 2-4 hours</li>
<li>Run DPO experiments in 1-2 hours</li>
<li>Iterate on ideas in a single day</li>
</ul>
<p>This speed enables a different kind of research - one where you can test hypotheses quickly and build on results iteratively.</p>
<h3 id="3-testing-hypotheses-quickly">3. <strong>Testing Hypotheses Quickly</strong></h3>
<p>NanoVLM-Lab is designed to test and implement quickly:</p>
<ul>
<li>YAML-based configuration (no code changes needed)</li>
<li>Comprehensive documentation</li>
<li>Example notebooks for every training approach</li>
<li>Integration with Weights &amp; Biases for experiment tracking</li>
</ul>
<hr />
<h2 id="getting-started">Getting Started</h2>
<p>Getting started is simple:</p>
<pre data-lang="bash" class="language-bash "><code class="language-bash" data-lang="bash"># Clone the repository
git clone https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;nanoVLM.git &amp;&amp; git clone https:&#x2F;&#x2F;github.com&#x2F;akash-kamalesh&#x2F;nanovlm-lab.git &amp;&amp; \
mv nanoVLM&#x2F; nanovlm-lab&#x2F;nanovlm &amp;&amp; \
mv nanovlm-lab&#x2F;rlvlm&#x2F;datasets.py nanovlm-lab&#x2F;rlvlm&#x2F;collators.py nanovlm-lab&#x2F;nanovlm&#x2F;data&#x2F;

# Install dependencies
pip install transformers datasets==4.4.1 gcsfs huggingface_hub trl matplotlib einops peft accelerate -qqq

# Run training
python rlvlm&#x2F;main.py --config configs&#x2F;sft_config.yaml
</code></pre>
<p>That's it. No complex setup, no mysterious hyperparameters. Just clear configuration files and working code.</p>
<hr />
<h2 id="what-s-included">What's Included</h2>
<ul>
<li><strong>Three Training Approaches</strong>: SFT, DPO, and GRPO with multiple variants</li>
<li><strong>Beginner-Friendly Docs</strong>: Start with the basics, dive into advanced topics as needed</li>
<li><strong>Example Notebooks</strong>: Interactive Jupyter notebooks for each training approach</li>
<li><strong>Production Features</strong>: Experiment tracking, checkpointing, mixed precision training</li>
<li><strong>LoRA Support</strong>: Parameter-efficient fine-tuning for all trainers</li>
<li><strong>Custom Reward Functions</strong>: Define your own reward signals for GRPO training</li>
</ul>
<hr />
<h2 id="the-bigger-picture">The Bigger Picture</h2>
<p>My research interest lies in building smaller and better performing foundation models. This is my second venture in building an open-source framework and I hope this project is of use to researchers who share the same opinion where <strong>efficiency matters, and small models deserve the same attention and tooling as large ones.</strong></p>
<p>Of course, frameworks like Unsloth and LLaMA Factory are excellent alternatives with their own strengths but they lack the ease of use and accessibility that NanoVLM-Lab offers. NanoVLM-Lab is specifically designed to explore how far we can push small-scale models—to understand their true potential and build a community around efficient, accessible training.</p>
<hr />
<h2 id="join-the-community">Join the Community</h2>
<p>NanoVLM-Lab is open-source and actively developed. Whether you want to:</p>
<ul>
<li>Train your own models</li>
<li>Contribute improvements</li>
<li>Explore new training techniques</li>
<li>Build on top of the framework</li>
</ul>
<p>...we'd love to have you involved.</p>
<p><strong>Check it out on GitHub</strong>: <a href="https://github.com/akash-kamalesh/nanovlm-lab">akash-kamalesh/nanovlm-lab</a></p>
<p><strong>Read the docs</strong>: Start with the <a href="https://github.com/akash-kamalesh/nanovlm-lab">main README</a>, then explore the <a href="https://github.com/akash-kamalesh/nanovlm-lab/tree/master/configs">Configuration Guide</a> and <a href="https://github.com/akash-kamalesh/nanovlm-lab/tree/master/examples">Training Approaches</a>.</p>
<hr />
<h2 id="citation">Citation</h2>
<p>If NanoVLM-Lab helps your research, please consider citing it:</p>
<pre data-lang="bibtex" class="language-bibtex "><code class="language-bibtex" data-lang="bibtex">@software{Kamalesh_NanoVLM-Lab_Compact_Training_2025,
author = {Kamalesh, Akash},
doi = {10.5281&#x2F;zenodo.17971745},
license = {MIT},
month = dec,
title = {{NanoVLM-Lab: Compact Training Framework for Small Vision-Language Models}},
url = {https:&#x2F;&#x2F;github.com&#x2F;akash-kamalesh&#x2F;nanovlm-lab},
version = {1.0.0},
year = {2025}
}
</code></pre>
<hr />
<h2 id="what-s-next">What's Next?</h2>
<p>This is just the beginning, and I'm actively listening to user feedback to shape the future of NanoVLM-Lab. Whether you've found a feature that would be incredibly useful, discovered a limitation you'd like addressed, or have ideas for improvements, I want to hear from you. If you believe a feature would be great to have, please raise an issue on GitHub-and if you're interested in contributing, you're more than welcome to submit a pull request. This project is meant to evolve with the community, and your input directly influences what gets built next.</p>
<hr />
<h2 id="final-thoughts">Final Thoughts</h2>
<p>A few years ago, I was frustrated by the limitations of my hardware. Today, I'm excited about what's possible when you focus on efficiency and smart design. NanoVLM-Lab is my contribution to changing how we think about model training.</p>
<p>I hope this project helps you validate your ideas, accelerate your research, and would love to see this project contribute to bigger, more amazing developments using small VLMs.</p>
<p><strong>- Akash Kamalesh</strong></p>
<hr />
<p><em>Have questions? Found a bug? Want to contribute? Open an issue or pull request on <a href="https://github.com/akash-kamalesh/nanovlm-lab">GitHub</a>.</em></p>

            </div>
            <footer class="blog-post-footer">
                <a href="https://akash-kamalesh.github.io/blog/" class="back-to-blog">← Back to Blog</a>
            </footer>
        </div>
    </article>

    </main>

    <!-- Footer -->
    <footer class="footer">
        <p>&copy; 2025 Akash Kamalesh. All rights reserved.</p>
    </footer>

    <script>
        function openAbstract(event, modalId = 'abstractModal') {
            event.preventDefault();
            const modal = document.getElementById(modalId);
            if (modal) {
                modal.classList.add('active');
            }
        }

        function closeAbstract(event, modalId = 'abstractModal') {
            const modal = document.getElementById(modalId);
            if (modal) {
                modal.classList.remove('active');
            }
        }

        function openProjectDescription(event, modalId) {
            event.preventDefault();
            const modal = document.getElementById(modalId);
            if (modal) {
                modal.classList.add('active');
            }
        }

        function closeProjectDescription(event, modalId) {
            const modal = document.getElementById(modalId);
            if (modal) {
                modal.classList.remove('active');
            }
        }

        let carouselAutoScrollInterval;
        let currentProjectIndex = 0;

        function scrollCarousel(direction) {
            const carousel = document.querySelector('.projects-carousel');
            if (carousel) {
                const cards = carousel.querySelectorAll('.project-card');
                const totalCards = cards.length;
                
                currentProjectIndex += direction;
                
                if (currentProjectIndex >= totalCards) {
                    currentProjectIndex = 0;
                } else if (currentProjectIndex < 0) {
                    currentProjectIndex = totalCards - 1;
                }
                
                const carouselWidth = carousel.offsetWidth;
                const cardWidth = carouselWidth;
                const gap = 32;
                const translateAmount = (cardWidth + gap) * currentProjectIndex;
                
                carousel.style.transform = `translateX(-${translateAmount}px)`;
            }
        }

        function startCarouselAutoScroll() {
            carouselAutoScrollInterval = setInterval(() => {
                scrollCarousel(1);
            }, 3000);
        }

        function resetCarouselAutoScroll() {
            clearInterval(carouselAutoScrollInterval);
            startCarouselAutoScroll();
        }

        document.addEventListener('DOMContentLoaded', function() {
            startCarouselAutoScroll();
            
            const carouselBtns = document.querySelectorAll('.carousel-btn');
            carouselBtns.forEach(btn => {
                btn.addEventListener('click', resetCarouselAutoScroll);
            });

            const carouselViewport = document.querySelector('.carousel-viewport');
            if (carouselViewport) {
                carouselViewport.addEventListener('mouseenter', function() {
                    clearInterval(carouselAutoScrollInterval);
                });
                carouselViewport.addEventListener('mouseleave', function() {
                    startCarouselAutoScroll();
                });
            }
        });

        // Close modal when pressing Escape key
        document.addEventListener('keydown', function(event) {
            if (event.key === 'Escape') {
                // Close all open modals
                const modals = document.querySelectorAll('.modal.active');
                modals.forEach(modal => {
                    modal.classList.remove('active');
                });
            }
        });
    </script>
</body>
</html>
