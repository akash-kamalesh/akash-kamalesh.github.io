{% extends "base.html" %}

{% block title %}{{ config.extra.name }} - Portfolio{% endblock %}

{% block content %}
    <!-- Hero Section -->
    <section class="hero">
        <div class="hero-content">
            <div class="profile-image-container">
                <img src="{{ get_url(path='profile.jpg') }}" alt="Profile" class="profile-image">
            </div>
            <h1 class="hero-name">{{ config.extra.name }}</h1>
            <p class="hero-subtitle">Software Engineer @Cisco | Prev. AI @IISc | Multimodal Reasoning and Reinforcement Learning</p>
        </div>
    </section>

    <!-- About Section -->
    <section id="about" class="about">
        <div class="container">
            <h2>About</h2>
            <div class="about-content">
                <p>
                    I'm a B.Tech Computer Science and Engineering graduate from PES University, Bengaluru. Currently conducting independent research on building scalable and efficient AI systems, with particular emphasis  <strong>representation learning, multimodal alignment, contrastive learning, and fine-tuning strategies for foundation models.</strong>
                </p>
                <p>
                    I've worked on methods like <strong>UnoLoRA</strong> for efficient multitask adaptation, explored grounding information better in vision-language models, and developed cross-lingual sparse <strong>Mixture of Experts</strong> architectures. My broader goal is to make foundation models more aligned, interpretable, and adaptable across tasks and modalities.
                </p>
                <p>
                    In addition to research, I've applied these ideas in real-world settings through roles at <strong>IISc</strong>, <strong>Swiggy</strong>, <strong>Nokia</strong>, and <strong>Cisco</strong>, where I worked on applying generative AI across practical domains such as finance and healthcare, building conversational recommender systems for personalized item suggestions and solving graph network problems.
                </p>
            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects" class="projects">
        <div class="container">
            <h2>Projects</h2>
            <div class="carousel-container">
                <button class="carousel-btn prev-btn" onclick="scrollCarousel(-1)">&#10094;</button>
                <div class="carousel-viewport">
                    <div class="projects-carousel">
                    <!-- Project Card 1 -->
                <div class="project-card">
                    <div class="project-image">
                        <img src="{{ get_url(path='nanovlm_lab_logo.png') }}" alt="Project 1">
                    </div>
                    <div class="project-content">
                        <h3 class="project-title">NanoVLM Lab</h3>
                        <div class="project-buttons">
                            <button class="project-btn about-btn" onclick="openProjectDescription(event, 'project1Modal')">ABOUT</button>
                            <a href="https://github.com" class="project-btn github-btn" target="_blank">
                                <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                                    <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v 3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                                </svg>
                            </a>
                        </div>
                    </div>
                </div>

                <!-- Project Card 2 -->
                <div class="project-card">
                    <div class="project-image">
                        <img src="{{ get_url(path='ntransformer_logo.png') }}" alt="Project 2">
                    </div>
                    <div class="project-content">
                        <h3 class="project-title">TNT: The Normalized Transformer</h3>
                        <div class="project-buttons">
                            <button class="project-btn about-btn" onclick="openProjectDescription(event, 'project2Modal')">ABOUT</button>
                            <a href="https://github.com/akash-kamalesh/normalized-transformer" class="project-btn github-btn" target="_blank">
                                <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                                    <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v 3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                                </svg>
                            </a>
                        </div>
                    </div>
                </div>

                <!-- Project Card 3 -->
                <div class="project-card">
                    <div class="project-image">
                        <img src="{{ get_url(path='baraat_logo.png') }}" alt="Project 3">
                    </div>
                    <div class="project-content">
                        <h3 class="project-title">Project Baraat</h3>
                        <div class="project-buttons">
                            <button class="project-btn about-btn" onclick="openProjectDescription(event, 'project3Modal')">ABOUT</button>
                            <a href="https://github.com/akash-kamalesh/Baraat" class="project-btn github-btn" target="_blank">
                                <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                                    <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v 3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                                </svg>
                            </a>
                        </div>
                    </div>
                </div>

                    </div>
                </div>
                <button class="carousel-btn next-btn" onclick="scrollCarousel(1)">&#10095;</button>
            </div>
        </div>
    </section>

    <!-- Project Description Modals -->
    <div id="project1Modal" class="modal" onclick="closeProjectDescription(event, 'project1Modal')">
        <div class="modal-content" onclick="event.stopPropagation()">
            <span class="close-btn" onclick="closeProjectDescription(null, 'project1Modal')">&times;</span>
            <h2>NanoVLM Lab</h2>
            <p>NanoVLM-Lab is an open-source training framework that democratizes vision-language model development for researchers and practitioners with limited computational resources. Built on HuggingFace's Transformers and TRL, it provides three powerful training approaches — Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO) — all optimized for efficient nanoVLM models. Whether you're working with a single T4 GPU or exploring cutting-edge alignment techniques, NanoVLM-Lab enables you to validate research ideas, build production-ready models, and iterate quickly without needing a supercomputer. The framework emphasizes accessibility, efficiency, and practical AI for everyone.</p>
        </div>
    </div>

    <div id="project2Modal" class="modal" onclick="closeProjectDescription(event, 'project2Modal')">
        <div class="modal-content" onclick="event.stopPropagation()">
            <span class="close-btn" onclick="closeProjectDescription(null, 'project2Modal')">&times;</span>
            <h2>Project Title 2</h2>
            <p>To be filled</p>
        </div>
    </div>

    <div id="project3Modal" class="modal" onclick="closeProjectDescription(event, 'project3Modal')">
        <div class="modal-content" onclick="event.stopPropagation()">
            <span class="close-btn" onclick="closeProjectDescription(null, 'project3Modal')">&times;</span>
            <h2>Project Title 3</h2>
            <p>To be filled</p>
        </div>
    </div>

    <!-- Publications Section -->
    <section id="publications" class="publications">
        <div class="container">
            <h2>Publications</h2>
            <div class="publications-list">
                <!-- Example Publication Card -->
                <div class="publication-card">
                    <div class="publication-image">
                        <img src="{{ get_url(path='isbi_preview.png') }}" alt="Publication thumbnail">
                    </div>
                    <div class="publication-content">
                        <div class="publication-badge">ISBI 2026 (UNDER REVIEW)</div>
                        <h3 class="publication-title">Pixel-FLAIR: Leveraging Anatomical Segmentation for Region-Specific Supervision in Retinal Foundational Vision-Language Models</h3>
                        <p class="publication-authors">Sasidhar Alavala, Akash Kamalesh, Chandra Sekhar Seelamantula</p>
                        <p class="publication-venue"><em>23rd International Symposium on Biomedical Imaging (ISBI 2026)</em></p>
                        <div class="publication-links">
                            <button class="pub-btn abs-btn" onclick="openAbstract(event, 'abstract3Modal')">ABS</button>
                            <a href="https://openreview.net/pdf?id=n6W0QkQBgw" class="pub-btn" target="_blank">PDF</a>
                        </div>
                    </div>
                </div>

                <div class="publication-card">
                    <div class="publication-image">
                        <img src="{{ get_url(path='unolora_preview.png') }}" alt="Publication thumbnail">
                    </div>
                    <div class="publication-content">
                        <div class="publication-badge">NeurIPS 2024</div>
                        <h3 class="publication-title">UnoLoRA: Single Low-Rank Adaptation for Efficient Multitask Fine-tuning</h3>
                        <p class="publication-authors">Akash Kamalesh, Anirudh Lakhotia, H S Nischal, Prerana Sanjay Kulkarni, and Gowri Srinivasa</p>
                        <p class="publication-venue"><em>Workshop on Fine-Tuning in Machine Learning at NeurIPS, 2024</em></p>
                        <div class="publication-links">
                            <button class="pub-btn abs-btn" onclick="openAbstract(event, 'abstractModal')">ABS</button>
                            <a href="https://openreview.net/pdf?id=n6W0QkQBgw" class="pub-btn" target="_blank">PDF</a>
                            <a href="https://neurips.cc/media/PosterPDFs/NeurIPS 2024/101671.png?t=1740678205.5010693" class="pub-btn" target="_blank">POSTER</a>
                            <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QUNjegoAAAAJ&citation_for_view=QUNjegoAAAAJ:u-x6o8ySG0sC" class="pub-btn" target="_blank">GOOGLE SCHOLAR</a>
                        </div>
                    </div>
                </div>

                <div class="publication-card">
                    <div class="publication-image">
                        <img src="{{ get_url(path='iceeng_preview.png') }}" alt="Publication thumbnail">
                    </div>
                    <div class="publication-content">
                        <div class="publication-badge">ICEENG 2025</div>
                        <h3 class="publication-title">Analysis of Sampling Strategies for Multi-Task Learning in Transformer Models</h3>
                        <p class="publication-authors">Anirudh Lakhotia, Akash Kamalesh, H S Nischal, Prerana Sanjay Kulkarni, and Gowri Srinivasa</p>
                        <p class="publication-venue"><em>2025 15th International Conference on Electrical Engineering (ICEENG)</em></p>
                        <div class="publication-links">
                            <button class="pub-btn abs-btn" onclick="openAbstract(event, 'abstract2Modal')">ABS</button>
                            <a href="https://ieeexplore.ieee.org/abstract/document/11031295" class="pub-btn" target="_blank">PDF</a>
                            <!-- <a href="https://neurips.cc/media/PosterPDFs/NeurIPS 2024/101671.png?t=1740678205.5010693" class="pub-btn" target="_blank">POSTER</a> -->
                            <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=QUNjegoAAAAJ&citation_for_view=QUNjegoAAAAJ:9yKSN-GCB0IC" class="pub-btn" target="_blank">GOOGLE SCHOLAR</a>
                        </div>
                    </div>
                </div>

                <!-- Add more publication cards as needed -->
            </div>
        </div>
    </section>

    <!-- Abstract Modal 1 - UnoLoRA -->
    <div id="abstractModal" class="modal" onclick="closeAbstract(event, 'abstractModal')">
        <div class="modal-content" onclick="event.stopPropagation()">
            <span class="close-btn" onclick="closeAbstract(null, 'abstractModal')">&times;</span>
            <h2>Abstract</h2>
            <p>
                Recent advances in Parameter-Efficient Fine-Tuning (PEFT) have shown Low-Rank Adaptation (LoRA) to be an effective implicit regularizer for large language models. Building on these findings, we propose UnoLoRA, a novel approach that leverages a single shared LoRA module for efficient multi-task learning. While existing methods typically use separate LoRA adaptations for each task, our approach demonstrates that a single shared adapter can effectively capture both task-specific and task-agnostic knowledge. We further introduce UnoLoRA*, an enhanced variant that employs a shared hypernetwork to generate task-specific embeddings, improving convergence and task adaptation. Our method significantly reduces trainable parameters to just 0.05% per task while maintaining competitive performance on the GLUE benchmark. Our analysis reveals that the A and B matrices in our shared LoRA adapter naturally develop complementary roles: A matrices capture generalizable features across tasks, while B matrices specialize in task-specific representations. Our results show that sharing a single LoRA adapter can achieve efficient multi-task learning while significantly reducing memory requirements, making it particularly valuable for resource-constrained applications.
            </p>
        </div>
    </div>

    <!-- Abstract Modal 2 - Sampling Strategies -->
    <div id="abstract2Modal" class="modal" onclick="closeAbstract(event, 'abstract2Modal')">
        <div class="modal-content" onclick="event.stopPropagation()">
            <span class="close-btn" onclick="closeAbstract(null, 'abstract2Modal')">&times;</span>
            <h2>Abstract</h2>
            <p>
                Multitask learning has emerged as a powerful paradigm for enhancing natural language understanding capabilities in large language models. However, the effectiveness of multitask learning heavily depends on the sampling strategy used to balance exposure to tasks of varying sizes and complexities during training. In this work, we conduct a comprehensive empirical analysis of three sampling strategies using T5-small on a subset of the GLUE benchmark: examples-proportional sampling, which samples based on raw dataset sizes; temperature-scaled sampling with T=10.0, which moderates size-based differences; and equal sampling, which gives uniform probability to all tasks. Our analysis examines the impact of these strategies on model performance, convergence patterns, task representation, and internal model dynamics through spectral analysis. Our results demonstrate that temperature-scaled sampling provides a strong balance between task representation and overall performance, while equal sampling achieves the highest average score across all tasks. Spectral analysis reveals that sampling strategies produce fundamentally different internal representations despite similar performance metrics, with higher temperatures promoting more uniform and stable layer behavior. We provide practical guidelines for selecting appropriate sampling strategies based on dataset characteristics, computational constraints, and specific training goals, contributing to more effective multitask learning approaches for language models.
            </p>
        </div>
    </div>

    <!-- Abstract Modal 3 - Pixel FLAIR -->
    <div id="abstract3Modal" class="modal" onclick="closeAbstract(event, 'abstract3Modal')">
        <div class="modal-content" onclick="event.stopPropagation()">
            <span class="close-btn" onclick="closeAbstract(null, 'abstract3Modal')">&times;</span>
            <h2>Abstract</h2>
            <p>
                Most current retinal foundational vision-language models do global image-text alignment, which often misses the subtle, localized features that are essential for early prescreening of retinal diseases. We propose Pixel-FLAIR, which offers better learned representations compared to FLAIR by incorporating a pixel-level text supervisory signal in addition to the standard image-level text supervisory signal. First, we use a segmentation model to find and extract key anatomical landmarks, which serve as anchors to automatically locate and extract clinically relevant sub-regions. We then pair these extracted image patches with their corresponding text labels. Then, the Pixel-FLAIR is fine-tuned using a contrastive loss where, for positive pairs, we modify the similarity score by combining the standard global image-text alignment score with a new, region-specific alignment score. Experiments show Pixel-FLAIR consistently outperforms the FLAIR baseline on classification tasks.
            </p>
        </div>
    </div>
{% endblock %}
